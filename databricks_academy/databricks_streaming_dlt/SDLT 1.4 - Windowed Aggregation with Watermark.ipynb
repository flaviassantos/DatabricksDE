{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "270beb3b-d7fd-420d-a39d-2655ee8bc00b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f432bb77-86b8-4624-b6a1-4c0c85428d51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": "--i18n-610f09e1-5f74-4b55-be94-2879bd22bbe9"
    }
   },
   "source": [
    "# Windowed Aggregation with Watermark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24c88250-ed47-4105-b9d1-b85691228021",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Objectives\n",
    "1. Build streaming DataFrames with time window aggregates with watermark\n",
    "1. Write streaming query results to Delta table using `update` mode and `forEachBatch()`\n",
    "1. Monitor the streaming query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "232e1d51-f5ab-489b-b0e0-e5791053a7ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Classes\n",
    "- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamReader.html\" target=\"_blank\">DataStreamReader</a>\n",
    "- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamWriter.html\" target=\"_blank\">DataStreamWriter</a>\n",
    "- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.StreamingQuery.html\" target=\"_blank\">StreamingQuery</a>\n",
    "- <a href=\"https://spark.apache.org/docs/3.5.2/structured-streaming-programming-guide.html#using-foreach-and-foreachbatch\" target=\"_blank\">foreachbatch</a>\n",
    "- <a href=\"https://docs.databricks.com/en/structured-streaming/delta-lake.html#upsert-from-streaming-queries-using-foreachbatch&language-python\" target=\"_blank\">update mode with foreachbatch</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "054bcd07-c233-40d6-88b9-dd92a00e37ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Includes/Classroom-Setup-04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a04cbdde-a3ee-49a4-ace8-11d987be7d23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": "--i18n-e06055f0-b40e-4997-b656-bf8cf94156a0"
    }
   },
   "source": [
    "\n",
    "## Build Streaming DataFrames\n",
    "\n",
    "Obtain an initial streaming DataFrame from a Delta-format file source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6bf478a3-c1bf-47ba-a770-efdbdbfd972d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import window, sum, col\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "parsed_df = (spark.readStream\n",
    "                    .load(DA.paths.events)\n",
    "                    .withColumn(\"event_timestamp\", (col(\"event_timestamp\") / 1e6).cast(\"timestamp\"))\n",
    "                    .withColumn(\"event_previous_timestamp\", (col(\"event_previous_timestamp\") / 1e6).cast(\"timestamp\"))\n",
    "\n",
    "                    # filter out zero revenue events\n",
    "                    .filter(\"ecommerce.purchase_revenue_in_usd IS NOT NULL AND ecommerce.purchase_revenue_in_usd != 0\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b758e617-fb8b-44e3-b546-6176a8f6e62e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(parsed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67eb31ce-d758-473a-a069-3882e0821a10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "windowed_df = (parsed_df\n",
    "                    # now add up revenues by city by 60 minute time window\n",
    "                    .withWatermark(eventTime=\"event_timestamp\", delayThreshold=\"90 minutes\")\n",
    "                    # group by city by hour\n",
    "                    .groupBy(window(timeColumn=\"event_timestamp\", windowDuration=\"60 minutes\"), \"geo.city\")\n",
    "                    .agg(sum(\"ecommerce.purchase_revenue_in_usd\").alias(\"total_revenue\"))\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8a3c3d7-c97c-4556-9814-ac5274ed5e55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(windowed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37c0119d-c056-4aca-8804-caef7183ab10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Write streaming results\n",
    "\n",
    "Let's explore a couple options for writing the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "850dcbb8-04d8-4702-b206-56de2a121666",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Write streaming results in `append` mode (option 1)\n",
    "\n",
    "In the below example, the sink table is appended with new rows from results table on each trigger.\n",
    "Think about how append mode writting data to sink. What will happen to records of those city whose hourly revenue got updated due to late arrival data. Would it be updated into the sink with \"append\" mode?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a15b466-30a0-4632-ba02-649f7400778a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_path = f\"{DA.paths.working_dir}/query_revenue_by_city_by_hour_append\"\n",
    "# Write the output of a streaming aggregation query into Delta table as updates.The implication of append output modes in the context of window aggregation and watermarks is that an aggregate can be produced only once and can not be updated. Therefore, once the aggregate is produced, the engine can delete the aggregate's state and thus keep the overall aggregation state bounded.\n",
    "windowed_query = (windowed_df.writeStream\n",
    "                  .queryName(\"query_revenue_by_city_by_hour_append\")\n",
    "                  .option(\"checkpointLocation\", checkpoint_path)\n",
    "                  .trigger(availableNow=True)\n",
    "                  .outputMode(\"append\")\n",
    "                  .table(\"revenue_by_city_by_hour_append\")\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "127f82d7-daad-4056-99da-3936d7716a9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM revenue_by_city_by_hour_append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2599f42-db62-425d-9526-0eebb664e9e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE HISTORY revenue_by_city_by_hour_append"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "640ed6f8-eb03-44c6-8911-5d3345900d3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": "--i18n-5a10eead-7c3b-41e6-bcbe-0d85095de1d7"
    }
   },
   "source": [
    "\n",
    "### Write streaming query results in `update` mode (option 2)\n",
    "\n",
    "Take the final streaming DataFrame (our result table) and write it to a Delta Table sink in `update` mode. This approach gives much greater control to the developer when it comes to updating the sink, albeit with greater complexity.\n",
    "\n",
    "**NOTE:** The syntax for Writing streaming results to a Delta table or dataset in `update` mode is a little different. It requires use of the `MERGE` command within a `forEachBatch()` function call. This also requires the target table to be pre-created.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "408081b8-b397-49af-8453-3275c45d597b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, LongType, ArrayType, TimestampType\n",
    "\n",
    "# Here we are creating the table with the same schema as the incoming dataframe\n",
    "schema = StructType([StructField('window',StructType([StructField('start', TimestampType(), True), \n",
    "                                                      StructField('end', TimestampType(), True)]), False), \n",
    "                     StructField('city', StringType(), True), \n",
    "                     StructField('total_revenue', DoubleType(), True)])\n",
    "\n",
    "empty_df = spark.createDataFrame([], schema=schema)\n",
    "\n",
    "empty_df.write.saveAsTable(name=\"revenue_by_city_by_hour\", mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39a08d26-7a23-4164-9c59-1ccdca58e281",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to upsert microBatchOutputDF into Delta table using merge\n",
    "\n",
    "def upsertToDelta(microBatchOutputDF, batchId):\n",
    "  # Set the dataframe to view name\n",
    "  microBatchOutputDF.createOrReplaceTempView(\"updates\")\n",
    "  # IMP: You have to use the SparkSession that has been used to define the `updates` dataframe\n",
    "\n",
    "  # In Databricks Runtime 10.5 and below, you must use the following:\n",
    "  # microBatchOutputDF._jdf.sparkSession().sql(\"\"\"\n",
    "  microBatchOutputDF.sparkSession.sql(\"\"\"\n",
    "    MERGE INTO revenue_by_city_by_hour t\n",
    "    USING updates s\n",
    "    ON t.window.start = s.window.start AND t.window.end = s.window.end AND t.city = s.city\n",
    "    WHEN MATCHED THEN UPDATE SET *\n",
    "    WHEN NOT MATCHED THEN INSERT *\n",
    "  \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4114e9a9-38f2-47fd-99d9-a116283609d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM revenue_by_city_by_hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ddbef7a9-303f-46ee-99ef-4b68534228f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Execute below code to write result table into delta table using **update** mode\n",
    "Dive deep into the query raw metrics and pay close attention to stateful operator section, see if you could identify **watermark** work in action and number of rows removed due to it's settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ccd08f9-ac22-4029-8ae1-cc45e0c1cdcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_path = f\"{DA.paths.working_dir}/query_revenue_by_city_by_hour\"\n",
    "\n",
    "# Write the output of a streaming aggregation query into Delta table as updates\n",
    "windowed_query = (windowed_df.writeStream\n",
    "                  .foreachBatch(upsertToDelta)\n",
    "                  .outputMode(\"update\")\n",
    "                  .queryName(\"query_revenue_by_city_by_hour\")\n",
    "                  .option(\"checkpointLocation\", checkpoint_path)\n",
    "                  .trigger(availableNow=True)\n",
    "                  .start()\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23eff6a2-25ff-4981-b169-d2918e8b1670",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM revenue_by_city_by_hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e37f4518-8bbe-46ee-9608-adcea72dd6e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE HISTORY revenue_by_city_by_hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f268203-3d68-4bd5-8a36-dc30cdb2696b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for s in spark.streams.active:\n",
    "  print(s.name)\n",
    "  s.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e93aebee-71ad-4cbb-9d5f-51bf0bb87bcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "&copy; 2025 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the \n",
    "<a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/><a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | \n",
    "<a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | \n",
    "<a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "SDLT 1.4 - Windowed Aggregation with Watermark",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}